{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from llama_index import SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "from llama_index import download_loader\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings import LangchainEmbedding\n",
    "\n",
    "from llama_index import  ServiceContext\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index import ServiceContext, LLMPredictor\n",
    "import datetime\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index import GPTVectorStoreIndex\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "JsonDataReader = download_loader(\"JsonDataReader\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################Variables, objects creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = \"ad OPANAI token....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameters for GPT3.5 model \n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "gpt_35_context = ServiceContext.from_defaults(llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.3))\n",
    "SYSTEM_PROMPT = \"\"\"You are an AI assistant that answers questions in a friendly manner, based on the given source documents. Here are some rules you always follow:\n",
    "- Generate human readable output, avoid creating output with gibberish text.\n",
    "- Generate only the requested output, don't include any other language before or after the requested output.\n",
    "- Never say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\n",
    "- Generate professional language typically used in business documents in North America.\n",
    "- Never generate offensive or foul language.\n",
    "\"\"\"\n",
    "\n",
    "query_wrapper_prompt = SimpleInputPrompt(\n",
    "    \"[INST]<<SYS>>\\n\" + SYSTEM_PROMPT + \"<</SYS>>\\n\\n{query_str}[/INST]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Loading index and querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI(model='gpt-3.5-turbo', temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#current_path = os.getcwd()\n",
    "path = os.path.dirname(os.getcwd())\n",
    "\n",
    "#path = os.path.dirname(current_path)\n",
    "service_context = ServiceContext.from_defaults(llm=llm)\n",
    "storage_context = StorageContext.from_defaults(persist_dir=path+\"/data/indexes/GPT3.5_index\")\n",
    "index = load_index_from_storage(storage_context, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUEST_WITH_SYSTEM-PROMPT:\n",
      "Act as a Eurostat's statistical assistant. Use the context data provided as descriptions of eurostat tables. Answer questions in a friendly manner, based on the given source documents as well as general statistical eurostat knowledge. Limit the output to the data related only to eurostat. Here are some rules you always follow: (1) Generate human readable output, avoid creating output with gibberish text. Answer directly with no additional introductory text (2) Generate only the requested output, don't include any other language before or after the requested output. (3) Generate professional language typically used in business documents in North America. (4) Never generate offensive or foul language. Please answer the folllowing question:which conutries had the highest growth on GDP in last 10 years and in which tables of eurostat data I can find these information? Provide also the names of the tables\n",
      "\n",
      "USER_PROMPT:\n",
      "which conutries had the highest growth on GDP in last 10 years and in which tables of eurostat data I can find these information? Provide also the names of the tables\n"
     ]
    }
   ],
   "source": [
    "# Create query\n",
    "\n",
    "system_prompt = \"Act as a Eurostat's statistical assistant. Use the context data provided as descriptions of eurostat tables. Answer questions in a friendly manner, based on the given source documents as well as general statistical eurostat knowledge. Limit the output to the data related only to eurostat. Here are some rules you always follow: (1) Generate human readable output, avoid creating output with gibberish text. Answer directly with no additional introductory text (2) Generate only the requested output, don't include any other language before or after the requested output. (3) Generate professional language typically used in business documents in North America. (4) Never generate offensive or foul language. Please answer the folllowing question:\"\n",
    "\n",
    "question = \"which conutries had the highest growth on GDP in last 10 years and in which tables of eurostat data I can find these information? Provide also the names of the tables\"\n",
    "\n",
    "full_request= system_prompt + question\n",
    "print (f\"REQUEST_WITH_SYSTEM-PROMPT:\\n{full_request}\")\n",
    "print (f\"\\nUSER_PROMPT:\\n{question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2024-01-07 16:42:45.155602\n",
      "RESPONSE_WITHOUGHT_SYSTEM-PROMPT:\n",
      "The context information does not provide any specific data or tables related to GDP growth in countries. Therefore, I cannot provide the names of the tables where this information can be found.\n",
      "end time: 2024-01-07 16:42:48.218548\n",
      "\n",
      "RESPONSE_WITH_SYSTEM-PROMPT:\n",
      "The countries with the highest growth in GDP in the last 10 years can be found in the eurostat tables. The specific tables that contain this information are not provided in the given context.\n",
      "end time: 2024-01-07 16:42:50.836482\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "print(\"start time:\", datetime.datetime.now())\n",
    "\n",
    "response_basic = query_engine.query(question)\n",
    "print (f\"RESPONSE_WITHOUGHT_SYSTEM-PROMPT:\\n{response_basic}\")\n",
    "print (\"end time:\", datetime.datetime.now())\n",
    "\n",
    "response_full = query_engine.query(full_request)\n",
    "print (f\"\\nRESPONSE_WITH_SYSTEM-PROMPT:\\n{response_full}\")\n",
    "print (\"end time:\", datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
